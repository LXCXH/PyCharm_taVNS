#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorchæ¨¡å‹è½¬æ¢ä¸ºTensorFlow Liteè„šæœ¬
ç”¨äºESP32-S3ä¸Šçš„TensorFlow Lite Microéƒ¨ç½²
åŒ…å«è½¬æ¢åçš„æ¨¡å‹æµ‹è¯•å’Œé¢„æµ‹å¯¹æ¯”
æ”¹è¿›ç‰ˆï¼šæé«˜è½¬æ¢å‡†ç¡®åº¦
"""

import torch
import torch.nn as nn
import tensorflow as tf
import numpy as np
import json
import pickle
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from model import taVNSNet
from data_processor import taVNSDataProcessor

class PyTorchToTFLiteConverter:
    """
    PyTorchæ¨¡å‹è½¬æ¢ä¸ºTensorFlow Liteçš„è½¬æ¢å™¨
    æ”¹è¿›ç‰ˆï¼šæé«˜è½¬æ¢å‡†ç¡®åº¦
    """
    
    def __init__(self, model_dir=None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
        """
        if model_dir is None:
            self.model_dir = self._find_latest_model_dir()
        else:
            self.model_dir = model_dir
            
        # æ¨¡å‹æ–‡ä»¶è·¯å¾„ - ä¼˜å…ˆä½¿ç”¨TFLiteä¼˜åŒ–ç‰ˆæœ¬
        tflite_optimized_path = os.path.join(self.model_dir, "tflite_optimized_model.pth")
        best_model_path = os.path.join(self.model_dir, "best_model.pth")
        
        if os.path.exists(tflite_optimized_path):
            self.model_path = tflite_optimized_path
            print(f"âœ“ ä½¿ç”¨TFLiteä¼˜åŒ–æ¨¡å‹: {tflite_optimized_path}")
        elif os.path.exists(best_model_path):
            self.model_path = best_model_path
            print(f"âœ“ ä½¿ç”¨æœ€ä½³æ¨¡å‹: {best_model_path}")
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {tflite_optimized_path} æˆ– {best_model_path}")
        
        self.config_path = os.path.join(self.model_dir, "training_config.json")
        self.data_processor_path = os.path.join(self.model_dir, "data_processor.pkl")
        
        # åˆ›å»ºä¸»è¾“å‡ºç›®å½•
        self.base_output_dir = "TFLite_Output"
        os.makedirs(self.base_output_dir, exist_ok=True)
        
        # ä¸ºæœ¬æ¬¡è½¬æ¢åˆ›å»ºä¸“ç”¨æ–‡ä»¶å¤¹
        self.conversion_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.output_dir = os.path.join(self.base_output_dir, f"conversion_{self.conversion_timestamp}")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {self.model_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _find_latest_model_dir(self):
        """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹ç›®å½•"""
        training_outputs_dir = "Training_Outputs"
        if not os.path.exists(training_outputs_dir):
            raise FileNotFoundError(f"è®­ç»ƒè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {training_outputs_dir}")
        
        # è·å–æ‰€æœ‰è®­ç»ƒè¾“å‡ºç›®å½•
        model_dirs = []
        for item in os.listdir(training_outputs_dir):
            item_path = os.path.join(training_outputs_dir, item)
            if os.path.isdir(item_path) and item.startswith("training_output_"):
                model_dirs.append(item_path)
        
        if not model_dirs:
            raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒè¾“å‡ºç›®å½•")
        
        # æŒ‰æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        model_dirs.sort(reverse=True)
        return model_dirs[0]
    
    def load_model_config(self):
        """åŠ è½½æ¨¡å‹é…ç½®"""
        print("\n=== åŠ è½½æ¨¡å‹é…ç½® ===")
        
        with open(self.config_path, 'r') as f:
            config = json.load(f)
        
        model_config = config['model_config']
        print(f"æ¨¡å‹é…ç½®: {model_config}")
        
        return model_config
    
    def load_pytorch_model(self, model_config):
        """åŠ è½½PyTorchæ¨¡å‹"""
        print("\n=== åŠ è½½PyTorchæ¨¡å‹ ===")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = taVNSNet(
            input_dim=model_config['input_dim'],
            param_dim=model_config['param_dim'],
            hidden_dim=model_config['hidden_dim'],
            num_layers=model_config.get('num_layers', 2),
            dropout=model_config.get('dropout', 0.2),
            num_individuals=model_config.get('num_individuals', 100)
        )
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        if 'model_state_dict' in checkpoint:
            model_state_dict = checkpoint['model_state_dict']
        else:
            model_state_dict = checkpoint
            
        model.load_state_dict(model_state_dict)
        model.to(self.device)
        model.eval()
        
        print("PyTorchæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯å’Œæƒé‡ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        
        # æ‰“å°æƒé‡ç»Ÿè®¡ä¿¡æ¯ç”¨äºè°ƒè¯•
        print("\nPyTorchæ¨¡å‹æƒé‡ç»Ÿè®¡:")
        for name, param in model.named_parameters():
            print(f"  {name}: {param.shape} | mean: {param.mean().item():.6f} | std: {param.std().item():.6f}")
        
        return model
    
    def load_data_processor(self):
        """åŠ è½½æ•°æ®å¤„ç†å™¨"""
        print("\n=== åŠ è½½æ•°æ®å¤„ç†å™¨ ===")
        
        with open(self.data_processor_path, 'rb') as f:
            data_processor = pickle.load(f)
        
        print("æ•°æ®å¤„ç†å™¨åŠ è½½æˆåŠŸ")
        return data_processor
    
    def create_tensorflow_model(self, pytorch_model, model_config):
        """åˆ›å»ºç­‰ä»·çš„TensorFlowæ¨¡å‹ - æ”¹è¿›ç‰ˆ"""
        print("\n=== åˆ›å»ºTensorFlowæ¨¡å‹ ===")
        
        # ä½¿ç”¨å‡½æ•°å¼APIåˆ›å»ºæ›´ç²¾ç¡®çš„æ¨¡å‹
        input_layer = tf.keras.layers.Input(shape=(model_config['input_dim'],), name='input')
        
        # è¡€ç³–åºåˆ—ç¼–ç å™¨ - ä½¿ç”¨ä¸PyTorchå®Œå…¨ç›¸åŒçš„ç»“æ„
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'] * 2, 
            activation='relu', 
            name='glucose_encoder_1',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(input_layer)
        
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'], 
            activation='relu', 
            name='glucose_encoder_2',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        # ç‰¹å¾æå–å™¨
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'], 
            activation='relu', 
            name='feature_extractor_1',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'] // 2, 
            activation='relu', 
            name='feature_extractor_2',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        # ä¸ªä½“é€‚åº”å±‚
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'] // 4, 
            activation='relu', 
            name='individual_adapter_1',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        # å‚æ•°é¢„æµ‹å¤´
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'] // 4, 
            activation='relu', 
            name='param_head_1',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        x = tf.keras.layers.Dense(
            model_config['hidden_dim'] // 8, 
            activation='relu', 
            name='param_head_2',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        output = tf.keras.layers.Dense(
            model_config['param_dim'], 
            activation='sigmoid', 
            name='param_output',
            kernel_initializer='glorot_uniform',
            bias_initializer='zeros'
        )(x)
        
        # åˆ›å»ºæ¨¡å‹
        tf_model = tf.keras.Model(inputs=input_layer, outputs=output, name='taVNSNet_TF')
        
        print("TensorFlowæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"TensorFlowæ¨¡å‹ç»“æ„:")
        tf_model.summary()
        
        return tf_model
    
    def transfer_weights_improved(self, pytorch_model, tf_model):
        """æ”¹è¿›çš„æƒé‡è½¬ç§»æ–¹æ³•"""
        print("\n=== æ”¹è¿›çš„æƒé‡è½¬ç§» ===")
        
        # è·å–PyTorchæ¨¡å‹çš„çŠ¶æ€å­—å…¸
        pytorch_state_dict = pytorch_model.state_dict()
        
        # æ”¹è¿›çš„æƒé‡æ˜ å°„ - ä½¿ç”¨å±‚åç§°è€Œä¸æ˜¯å˜é‡å
        layer_mapping = {
            # ç¼–ç å™¨å±‚
            ('glucose_encoder.0.weight', 'glucose_encoder.0.bias'): 'glucose_encoder_1',
            ('glucose_encoder.2.weight', 'glucose_encoder.2.bias'): 'glucose_encoder_2',
            
            # ç‰¹å¾æå–å™¨å±‚
            ('feature_extractor.0.weight', 'feature_extractor.0.bias'): 'feature_extractor_1',
            ('feature_extractor.2.weight', 'feature_extractor.2.bias'): 'feature_extractor_2',
            
            # ä¸ªä½“é€‚åº”å±‚
            ('individual_adapter.0.weight', 'individual_adapter.0.bias'): 'individual_adapter_1',
            
            # å‚æ•°é¢„æµ‹å¤´
            ('param_head.0.weight', 'param_head.0.bias'): 'param_head_1',
            ('param_head.2.weight', 'param_head.2.bias'): 'param_head_2',
            ('param_head.4.weight', 'param_head.4.bias'): 'param_output'
        }
        
        transferred_count = 0
        total_layers = len(layer_mapping)
        
        for (pytorch_weight_name, pytorch_bias_name), tf_layer_name in layer_mapping.items():
            # è·å–TensorFlowå±‚
            tf_layer = tf_model.get_layer(tf_layer_name)
            
            # è½¬ç§»æƒé‡
            if pytorch_weight_name in pytorch_state_dict and pytorch_bias_name in pytorch_state_dict:
                pytorch_weight = pytorch_state_dict[pytorch_weight_name].cpu().numpy()
                pytorch_bias = pytorch_state_dict[pytorch_bias_name].cpu().numpy()
                
                # PyTorchæƒé‡æ˜¯ [out_features, in_features]ï¼Œéœ€è¦è½¬ç½®ä¸º [in_features, out_features]
                pytorch_weight_transposed = pytorch_weight.T
                
                # æ£€æŸ¥å½¢çŠ¶
                tf_weights = tf_layer.get_weights()
                if len(tf_weights) == 2:  # æƒé‡å’Œåç½®
                    tf_weight_shape = tf_weights[0].shape
                    tf_bias_shape = tf_weights[1].shape
                    
                    if pytorch_weight_transposed.shape == tf_weight_shape and pytorch_bias.shape == tf_bias_shape:
                        # è®¾ç½®æƒé‡
                        tf_layer.set_weights([pytorch_weight_transposed, pytorch_bias])
                        transferred_count += 1
                        print(f"âœ“ æˆåŠŸè½¬ç§»: {tf_layer_name}")
                        print(f"  æƒé‡å½¢çŠ¶: {pytorch_weight_transposed.shape}")
                        print(f"  åç½®å½¢çŠ¶: {pytorch_bias.shape}")
                        print(f"  æƒé‡ç»Ÿè®¡: mean={pytorch_weight_transposed.mean():.6f}, std={pytorch_weight_transposed.std():.6f}")
                    else:
                        print(f"âœ— å½¢çŠ¶ä¸åŒ¹é…: {tf_layer_name}")
                        print(f"  PyTorchæƒé‡: {pytorch_weight_transposed.shape} vs TFæƒé‡: {tf_weight_shape}")
                        print(f"  PyTorchåç½®: {pytorch_bias.shape} vs TFåç½®: {tf_bias_shape}")
                else:
                    print(f"âœ— TensorFlowå±‚æƒé‡æ•°é‡å¼‚å¸¸: {tf_layer_name}")
            else:
                print(f"âœ— æœªæ‰¾åˆ°PyTorchæƒé‡: {pytorch_weight_name} æˆ– {pytorch_bias_name}")
        
        print(f"\næƒé‡è½¬ç§»å®Œæˆ: {transferred_count}/{total_layers} å±‚")
        
        # éªŒè¯æƒé‡è½¬ç§»
        if transferred_count == total_layers:
            print("âœ“ æ‰€æœ‰æƒé‡è½¬ç§»æˆåŠŸ")
            self._verify_weight_transfer(pytorch_model, tf_model)
        else:
            print("âš  éƒ¨åˆ†æƒé‡è½¬ç§»å¤±è´¥")
        
        return transferred_count == total_layers
    
    def _verify_weight_transfer(self, pytorch_model, tf_model):
        """éªŒè¯æƒé‡è½¬ç§»çš„æ­£ç¡®æ€§"""
        print("\n=== éªŒè¯æƒé‡è½¬ç§» ===")
        
        # ç”Ÿæˆéšæœºæµ‹è¯•è¾“å…¥
        test_input = np.random.randn(1, 12).astype(np.float32)
        
        # PyTorché¢„æµ‹
        pytorch_model.eval()
        with torch.no_grad():
            pytorch_input = torch.FloatTensor(test_input).to(self.device)
            pytorch_output = pytorch_model(pytorch_input).cpu().numpy()
        
        # TensorFlowé¢„æµ‹
        tf_output = tf_model(test_input).numpy()
        
        # è®¡ç®—å·®å¼‚
        max_diff = np.max(np.abs(pytorch_output - tf_output))
        mean_diff = np.mean(np.abs(pytorch_output - tf_output))
        
        print(f"PyTorchè¾“å‡º: {pytorch_output[0]}")
        print(f"TensorFlowè¾“å‡º: {tf_output[0]}")
        print(f"æœ€å¤§å·®å¼‚: {max_diff:.8f}")
        print(f"å¹³å‡å·®å¼‚: {mean_diff:.8f}")
        
        if max_diff < 1e-5:
            print("âœ“ æƒé‡è½¬ç§»éªŒè¯é€šè¿‡ (å·®å¼‚ < 1e-5)")
            return True
        elif max_diff < 1e-3:
            print("âš  æƒé‡è½¬ç§»åŸºæœ¬æ­£ç¡® (å·®å¼‚ < 1e-3)")
            return True
        else:
            print("âœ— æƒé‡è½¬ç§»å¯èƒ½æœ‰é—®é¢˜ (å·®å¼‚è¾ƒå¤§)")
            return False
    
    def convert_to_tflite(self, tf_model, model_config, use_quantization=False):
        """è½¬æ¢ä¸ºTensorFlow Liteæ¨¡å‹ - æ”¹è¿›ç‰ˆ
        
        Args:
            tf_model: TensorFlowæ¨¡å‹
            model_config: æ¨¡å‹é…ç½®
            use_quantization: æ˜¯å¦ä½¿ç”¨é‡åŒ–ä¼˜åŒ– (é»˜è®¤Falseä¸ºFloat32éé‡åŒ–ï¼ŒTrueä¸ºé‡åŒ–)
        """
        optimization_type = "é‡åŒ–ä¼˜åŒ–" if use_quantization else "Float32éé‡åŒ–"
        print(f"\n=== è½¬æ¢ä¸ºTensorFlow Lite ({optimization_type}) ===")
        
        # åˆ›å»ºè½¬æ¢å™¨
        converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
        
        # è®¾ç½®è¾“å…¥å½¢çŠ¶
        input_shape = [1, model_config['input_dim']]
        
        if use_quantization:
            # é‡åŒ–æ¨¡å¼ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„ä¼˜åŒ–
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # åªä½¿ç”¨å†…ç½®æ“ä½œï¼ˆé€‚åˆå¾®æ§åˆ¶å™¨ï¼‰
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
            
            # æ·»åŠ ä»£è¡¨æ€§æ•°æ®é›†ä»¥æé«˜é‡åŒ–è´¨é‡
            def representative_dataset():
                print("ç”Ÿæˆä»£è¡¨æ€§æ•°æ®é›†ç”¨äºé‡åŒ–...")
                
                # ä½¿ç”¨çœŸå®çš„è®­ç»ƒæ•°æ®æ ·æœ¬
                try:
                    samples = self._generate_test_samples(self.data_processor)
                    for i, sample in enumerate(samples):
                        # æ ‡å‡†åŒ–è¾“å…¥
                        normalized_sample = self.data_processor.glucose_scaler.transform([sample])
                        yield [normalized_sample.astype(np.float32)]
                        if i >= 99:  # é™åˆ¶æ ·æœ¬æ•°é‡
                            break
                except:
                    # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨éšæœºæ•°æ®
                    for _ in range(100):
                        data = np.random.randn(1, model_config['input_dim']).astype(np.float32)
                        yield [data]
            
            converter.representative_dataset = representative_dataset
        else:
            # Float32éé‡åŒ–æ¨¡å¼ï¼šä¸“ä¸ºESP32-S3 TFLite Microä¼˜åŒ–
            converter.optimizations = []  # ä¸è¿›è¡Œä»»ä½•ä¼˜åŒ–
            converter.target_spec.supported_types = [tf.float32]  # åªæ”¯æŒfloat32
            print("ä½¿ç”¨Float32éé‡åŒ–æ¨¡å¼ï¼Œä¸“ä¸ºESP32-S3ä¼˜åŒ–")
        
        # è®¾ç½®è¾“å…¥è¾“å‡ºç±»å‹
        converter.inference_input_type = tf.float32
        converter.inference_output_type = tf.float32
        
        try:
            # è½¬æ¢æ¨¡å‹
            tflite_model = converter.convert()
            print("TensorFlow Liteè½¬æ¢æˆåŠŸ")
            
            # ä¿å­˜æ¨¡å‹ - æ ¹æ®é‡åŒ–é€‰é¡¹é€‰æ‹©æ–‡ä»¶å
            model_suffix = "float32" if not use_quantization else "improved"
            tflite_filename = f"tavns_model_{model_suffix}.tflite"
            tflite_path = os.path.join(self.output_dir, tflite_filename)
            
            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)
            
            print(f"TFLiteæ¨¡å‹å·²ä¿å­˜: {tflite_path}")
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            improvements = [
                'improved_weight_transfer',
                'weight_transfer_verification'
            ]
            
            if use_quantization:
                improvements.extend(['representative_dataset', 'quantization_optimization'])
            else:
                improvements.extend(['float32_non_quantized', 'esp32_optimized'])
            
            model_info = {
                'model_path': tflite_path,
                'input_shape': input_shape,
                'output_shape': [1, model_config['param_dim']],
                'model_size_bytes': len(tflite_model),
                'model_size_kb': len(tflite_model) / 1024,
                'conversion_time': self.conversion_timestamp,
                'original_model_dir': self.model_dir,
                'optimization_type': optimization_type,
                'quantized': use_quantization,
                'esp32_compatible': not use_quantization,
                'improvements': improvements
            }
            
            info_path = os.path.join(self.output_dir, f"model_info.json")
            with open(info_path, 'w') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            print(f"æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_path}")
            print(f"æ¨¡å‹å¤§å°: {len(tflite_model):,} å­—èŠ‚ ({len(tflite_model)/1024:.1f} KB)")
            
            return tflite_path, model_info
            
        except Exception as e:
            print(f"TensorFlow Liteè½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def test_model_comparison(self, pytorch_model, tflite_path, data_processor, model_config):
        """æµ‹è¯•PyTorchæ¨¡å‹å’ŒTFLiteæ¨¡å‹çš„é¢„æµ‹å¯¹æ¯” - æ”¹è¿›ç‰ˆ"""
        print("\n=== æ¨¡å‹é¢„æµ‹å¯¹æ¯”æµ‹è¯• ===")
        
        if tflite_path is None:
            print("TFLiteæ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¹æ¯”æµ‹è¯•")
            return
        
        # åŠ è½½TFLiteæ¨¡å‹
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        
        # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        print(f"TFLiteè¾“å…¥å½¢çŠ¶: {input_details[0]['shape']}")
        print(f"TFLiteè¾“å‡ºå½¢çŠ¶: {output_details[0]['shape']}")
        print(f"TFLiteè¾“å…¥ç±»å‹: {input_details[0]['dtype']}")
        print(f"TFLiteè¾“å‡ºç±»å‹: {output_details[0]['dtype']}")
        
        # ç”Ÿæˆæµ‹è¯•æ ·æœ¬
        test_samples = self._generate_test_samples(data_processor)
        
        print(f"\nå¼€å§‹å¯¹æ¯”æµ‹è¯• ({len(test_samples)} ä¸ªæ ·æœ¬)...")
        
        pytorch_predictions = []
        tflite_predictions = []
        mae_errors = []
        normalized_mae_errors = []  # æ ‡å‡†åŒ–ç©ºé—´çš„è¯¯å·®
        
        for i, (glucose_seq, description) in enumerate(test_samples):
            # PyTorché¢„æµ‹
            pytorch_model.eval()
            with torch.no_grad():
                glucose_tensor = torch.FloatTensor(glucose_seq).unsqueeze(0).to(self.device)
                pytorch_pred_norm = pytorch_model(glucose_tensor).cpu().numpy()[0]
            
            # TFLiteé¢„æµ‹
            glucose_input = glucose_seq.astype(np.float32).reshape(1, -1)
            interpreter.set_tensor(input_details[0]['index'], glucose_input)
            interpreter.invoke()
            tflite_pred_norm = interpreter.get_tensor(output_details[0]['index'])[0]
            
            # è®¡ç®—æ ‡å‡†åŒ–ç©ºé—´çš„è¯¯å·®
            norm_mae = np.mean(np.abs(pytorch_pred_norm - tflite_pred_norm))
            normalized_mae_errors.append(norm_mae)
            
            # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
            pytorch_pred_orig = data_processor.inverse_transform_params(pytorch_pred_norm.reshape(1, -1))[0]
            tflite_pred_orig = data_processor.inverse_transform_params(tflite_pred_norm.reshape(1, -1))[0]
            
            # è®¡ç®—åŸå§‹ç©ºé—´çš„è¯¯å·®
            mae = np.mean(np.abs(pytorch_pred_orig - tflite_pred_orig))
            mae_errors.append(mae)
            
            pytorch_predictions.append(pytorch_pred_orig)
            tflite_predictions.append(tflite_pred_orig)
            
            # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
            print(f"\n--- æµ‹è¯•æ ·æœ¬ {i+1}: {description} ---")
            glucose_orig = data_processor.inverse_transform_glucose(glucose_seq.reshape(1, -1))[0]
            print(f"è¾“å…¥è¡€ç³–: {glucose_orig.round(2)}")
            print(f"PyTorché¢„æµ‹: [é¢‘ç‡={pytorch_pred_orig[0]:.2f}Hz, ç”µæµ={pytorch_pred_orig[1]:.2f}mA, "
                  f"æ—¶é•¿={pytorch_pred_orig[2]:.1f}min, è„‰å®½={pytorch_pred_orig[3]:.0f}Î¼s, å‘¨æœŸ={pytorch_pred_orig[4]:.1f}å‘¨]")
            print(f"TFLiteé¢„æµ‹:  [é¢‘ç‡={tflite_pred_orig[0]:.2f}Hz, ç”µæµ={tflite_pred_orig[1]:.2f}mA, "
                  f"æ—¶é•¿={tflite_pred_orig[2]:.1f}min, è„‰å®½={tflite_pred_orig[3]:.0f}Î¼s, å‘¨æœŸ={tflite_pred_orig[4]:.1f}å‘¨]")
            print(f"æ ‡å‡†åŒ–ç©ºé—´MAE: {norm_mae:.6f}")
            print(f"åŸå§‹ç©ºé—´MAE: {mae:.4f}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_mae = np.mean(mae_errors)
        max_mae = np.max(mae_errors)
        min_mae = np.min(mae_errors)
        
        overall_norm_mae = np.mean(normalized_mae_errors)
        max_norm_mae = np.max(normalized_mae_errors)
        min_norm_mae = np.min(normalized_mae_errors)
        
        print(f"\n=== å¯¹æ¯”æµ‹è¯•æ€»ç»“ ===")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}")
        print(f"æ ‡å‡†åŒ–ç©ºé—´è¯¯å·®:")
        print(f"  å¹³å‡MAE: {overall_norm_mae:.6f}")
        print(f"  æœ€å¤§MAE: {max_norm_mae:.6f}")
        print(f"  æœ€å°MAE: {min_norm_mae:.6f}")
        print(f"åŸå§‹ç©ºé—´è¯¯å·®:")
        print(f"  å¹³å‡MAE: {overall_mae:.4f}")
        print(f"  æœ€å¤§MAE: {max_mae:.4f}")
        print(f"  æœ€å°MAE: {min_mae:.4f}")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_results = {
            'test_samples_count': len(test_samples),
            'normalized_space': {
                'overall_mae': float(overall_norm_mae),
                'max_mae': float(max_norm_mae),
                'min_mae': float(min_norm_mae),
                'mae_errors': [float(x) for x in normalized_mae_errors]
            },
            'original_space': {
                'overall_mae': float(overall_mae),
                'max_mae': float(max_mae),
                'min_mae': float(min_mae),
                'mae_errors': [float(x) for x in mae_errors]
            },
            'pytorch_predictions': [x.tolist() for x in pytorch_predictions],
            'tflite_predictions': [x.tolist() for x in tflite_predictions]
        }
        
        comparison_path = os.path.join(self.output_dir, f"model_comparison.json")
        with open(comparison_path, 'w') as f:
            json.dump(comparison_results, f, indent=2)
        
        print(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_path}")
        
        # åˆ¤æ–­è½¬æ¢è´¨é‡ - åŸºäºæ ‡å‡†åŒ–ç©ºé—´çš„è¯¯å·®
        if overall_norm_mae < 1e-4:
            print("âœ“ è½¬æ¢è´¨é‡: ä¼˜ç§€ (æ ‡å‡†åŒ–MAE < 1e-4)")
            quality_status = "excellent"
        elif overall_norm_mae < 1e-3:
            print("âœ“ è½¬æ¢è´¨é‡: è‰¯å¥½ (æ ‡å‡†åŒ–MAE < 1e-3)")
            quality_status = "good"
        elif overall_norm_mae < 1e-2:
            print("âš  è½¬æ¢è´¨é‡: ä¸€èˆ¬ (æ ‡å‡†åŒ–MAE < 1e-2)")
            quality_status = "fair"
        else:
            print("âœ— è½¬æ¢è´¨é‡: è¾ƒå·® (æ ‡å‡†åŒ–MAE >= 1e-2)")
            quality_status = "poor"
        
        # ä¿å­˜è´¨é‡çŠ¶æ€åˆ°ç»“æœä¸­
        comparison_results['conversion_quality'] = quality_status
        
        # é‡æ–°ä¿å­˜åŒ…å«è´¨é‡çŠ¶æ€çš„ç»“æœ
        with open(comparison_path, 'w') as f:
            json.dump(comparison_results, f, indent=2)
        
        return comparison_results
    
    def _generate_test_samples(self, data_processor):
        """ç”Ÿæˆæµ‹è¯•æ ·æœ¬"""
        test_samples = []
        
        # æµ‹è¯•æ ·æœ¬1ï¼šæ­£å¸¸ç©ºè…¹è¡€ç³–
        glucose_1 = np.array([5.2, 5.1, 5.3, 5.0, 5.2, 5.4, 5.1, 5.0, 5.3, 5.2, 5.1, 5.0])
        glucose_1_norm = data_processor.glucose_scaler.transform(glucose_1.reshape(1, -1))[0]
        test_samples.append((glucose_1_norm, "æ­£å¸¸ç©ºè…¹è¡€ç³–"))
        
        # æµ‹è¯•æ ·æœ¬2ï¼šç³–å°¿ç—…é«˜è¡€ç³–
        glucose_2 = np.array([15.2, 16.1, 15.8, 16.5, 15.9, 16.2, 15.7, 16.0, 15.8, 16.1, 15.9, 16.0])
        glucose_2_norm = data_processor.glucose_scaler.transform(glucose_2.reshape(1, -1))[0]
        test_samples.append((glucose_2_norm, "ç³–å°¿ç—…é«˜è¡€ç³–"))
        
        # æµ‹è¯•æ ·æœ¬3ï¼šé¤åè¡€ç³–å‡é«˜
        glucose_3 = np.array([7.0, 8.5, 10.2, 12.1, 11.8, 10.5, 9.2, 8.5, 7.8, 7.2, 6.9, 6.8])
        glucose_3_norm = data_processor.glucose_scaler.transform(glucose_3.reshape(1, -1))[0]
        test_samples.append((glucose_3_norm, "é¤åè¡€ç³–å‡é«˜"))
        
        # æµ‹è¯•æ ·æœ¬4ï¼šè¡€ç³–æ³¢åŠ¨å¤§
        glucose_4 = np.array([8.0, 12.5, 6.8, 14.2, 7.5, 11.8, 9.2, 13.1, 8.5, 10.8, 9.5, 11.2])
        glucose_4_norm = data_processor.glucose_scaler.transform(glucose_4.reshape(1, -1))[0]
        test_samples.append((glucose_4_norm, "è¡€ç³–æ³¢åŠ¨è¾ƒå¤§"))
        
        # æµ‹è¯•æ ·æœ¬5ï¼šä½è¡€ç³–
        glucose_5 = np.array([3.8, 3.5, 3.9, 3.6, 3.7, 3.8, 3.5, 3.6, 3.9, 3.7, 3.8, 3.6])
        glucose_5_norm = data_processor.glucose_scaler.transform(glucose_5.reshape(1, -1))[0]
        test_samples.append((glucose_5_norm, "ä½è¡€ç³–çŠ¶æ€"))
        
        return test_samples
    
    def run_conversion(self, use_quantization=False):
        """è¿è¡Œå®Œæ•´çš„è½¬æ¢æµç¨‹ - æ”¹è¿›ç‰ˆ
        
        Args:
            use_quantization: æ˜¯å¦ä½¿ç”¨é‡åŒ–ä¼˜åŒ– (é»˜è®¤Falseä¸ºFloat32éé‡åŒ–ï¼ŒTrueä¸ºé‡åŒ–)
        """
        optimization_desc = "é‡åŒ–ä¼˜åŒ–" if use_quantization else "Float32éé‡åŒ–"
        print(f"=== PyTorchåˆ°TensorFlow Liteè½¬æ¢å¼€å§‹ (æ”¹è¿›ç‰ˆ - {optimization_desc}) ===")
        
        try:
            # 1. åŠ è½½æ¨¡å‹é…ç½®
            model_config = self.load_model_config()
            
            # 2. åŠ è½½PyTorchæ¨¡å‹
            pytorch_model = self.load_pytorch_model(model_config)
            
            # 3. åŠ è½½æ•°æ®å¤„ç†å™¨
            data_processor = self.load_data_processor()
            
            # 4. åˆ›å»ºTensorFlowæ¨¡å‹
            tf_model = self.create_tensorflow_model(pytorch_model, model_config)
            
            # 5. æ”¹è¿›çš„æƒé‡è½¬ç§»
            weight_transfer_success = self.transfer_weights_improved(pytorch_model, tf_model)
            
            if not weight_transfer_success:
                print("âš  æƒé‡è½¬ç§»å¤±è´¥ï¼Œè½¬æ¢å¯èƒ½ä¸å‡†ç¡®")
                # é‡å‘½åæ–‡ä»¶å¤¹ä¸ºå¤±è´¥çŠ¶æ€
                failed_dir = os.path.join(self.base_output_dir, f"conversion_{self.conversion_timestamp}_FAILED")
                os.rename(self.output_dir, failed_dir)
                print(f"è½¬æ¢å¤±è´¥ï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {failed_dir}")
                return False
            
            # 6. è½¬æ¢ä¸ºTFLite
            tflite_path, model_info = self.convert_to_tflite(tf_model, model_config, use_quantization)
            
            # 7. æ¨¡å‹å¯¹æ¯”æµ‹è¯•
            if tflite_path:
                comparison_results = self.test_model_comparison(
                    pytorch_model, tflite_path, data_processor, model_config
                )
                
                # æ ¹æ®è½¬æ¢è´¨é‡é‡å‘½åæ–‡ä»¶å¤¹
                quality_status = comparison_results['conversion_quality']
                quality_suffix = {
                    'excellent': 'EXCELLENT',
                    'good': 'GOOD', 
                    'fair': 'FAIR',
                    'poor': 'POOR'
                }
                
                final_dir = os.path.join(self.base_output_dir, 
                                       f"conversion_{self.conversion_timestamp}_{quality_suffix[quality_status]}")
                os.rename(self.output_dir, final_dir)
                
                # åˆ›å»ºè½¬æ¢æ‘˜è¦æ–‡ä»¶
                self._create_conversion_summary(final_dir, model_config, model_info, comparison_results)
                
                print(f"\n=== è½¬æ¢å®Œæˆ ===")
                print(f"TFLiteæ¨¡å‹: {os.path.join(final_dir, 'tavns_model_improved.tflite')}")
                print(f"æ¨¡å‹å¤§å°: {model_info['model_size_bytes']:,} å­—èŠ‚")
                print(f"æ ‡å‡†åŒ–ç©ºé—´é¢„æµ‹è¯¯å·®: {comparison_results['normalized_space']['overall_mae']:.6f}")
                print(f"åŸå§‹ç©ºé—´é¢„æµ‹è¯¯å·®: {comparison_results['original_space']['overall_mae']:.4f}")
                print(f"è½¬æ¢è´¨é‡: {quality_status.upper()}")
                print(f"æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {final_dir}")
                
                return True
            else:
                print("è½¬æ¢å¤±è´¥")
                # é‡å‘½åæ–‡ä»¶å¤¹ä¸ºå¤±è´¥çŠ¶æ€
                failed_dir = os.path.join(self.base_output_dir, f"conversion_{self.conversion_timestamp}_FAILED")
                os.rename(self.output_dir, failed_dir)
                print(f"è½¬æ¢å¤±è´¥ï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {failed_dir}")
                return False
                
        except Exception as e:
            print(f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            # é‡å‘½åæ–‡ä»¶å¤¹ä¸ºé”™è¯¯çŠ¶æ€
            error_dir = os.path.join(self.base_output_dir, f"conversion_{self.conversion_timestamp}_ERROR")
            if os.path.exists(self.output_dir):
                os.rename(self.output_dir, error_dir)
                print(f"è½¬æ¢å‡ºé”™ï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {error_dir}")
            return False
    
    def _create_conversion_summary(self, output_dir, model_config, model_info, comparison_results):
        """åˆ›å»ºè½¬æ¢æ‘˜è¦æ–‡ä»¶"""
        summary = {
            'conversion_info': {
                'timestamp': self.conversion_timestamp,
                'source_model': self.model_dir,
                'conversion_quality': comparison_results['conversion_quality'],
                'success': True
            },
            'model_details': {
                'input_dim': model_config['input_dim'],
                'param_dim': model_config['param_dim'],
                'hidden_dim': model_config['hidden_dim'],
                'model_size_bytes': model_info['model_size_bytes'],
                'model_size_kb': round(model_info['model_size_bytes'] / 1024, 1)
            },
            'accuracy_metrics': {
                'normalized_space_mae': comparison_results['normalized_space']['overall_mae'],
                'original_space_mae': comparison_results['original_space']['overall_mae'],
                'test_samples_count': comparison_results['test_samples_count']
            },
            'output_files': {
                'tflite_model': 'tavns_model_improved.tflite',
                'model_info': 'model_info.json',
                'comparison_results': 'model_comparison.json',
                'summary': 'conversion_summary.json'
            }
        }
        
        summary_path = os.path.join(output_dir, 'conversion_summary.json')
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"è½¬æ¢æ‘˜è¦å·²ä¿å­˜: {summary_path}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='taVNSæ¨¡å‹TensorFlow Liteè½¬æ¢å·¥å…· (æ”¹è¿›ç‰ˆ)')
    parser.add_argument('--float32', action='store_true', 
                       help='ä½¿ç”¨Float32éé‡åŒ–æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œä¸“ä¸ºESP32-S3ä¼˜åŒ–ï¼‰')
    parser.add_argument('--quantized', action='store_true', 
                       help='ä½¿ç”¨é‡åŒ–ä¼˜åŒ–æ¨¡å¼')
    
    args = parser.parse_args()
    
    # ç¡®å®šä½¿ç”¨çš„è½¬æ¢æ¨¡å¼
    if args.quantized:
        use_quantization = True
        mode_desc = "é‡åŒ–ä¼˜åŒ–æ¨¡å¼"
    else:
        use_quantization = False
        mode_desc = "Float32éé‡åŒ– - ä¸“ä¸ºESP32-S3 TFLite Microä¼˜åŒ–ï¼ˆé»˜è®¤ï¼‰"
    
    print("=== taVNSæ¨¡å‹TensorFlow Liteè½¬æ¢å·¥å…· (æ”¹è¿›ç‰ˆ) ===")
    print(f"ğŸ”§ è½¬æ¢æ¨¡å¼: {mode_desc}")
    
    # åˆ›å»ºè½¬æ¢å™¨
    converter = PyTorchToTFLiteConverter()
    
    # è¿è¡Œè½¬æ¢
    success = converter.run_conversion(use_quantization)
    
    if success:
        print("\nâœ“ è½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"è¾“å‡ºæ–‡ä»¶ä½äº: {converter.output_dir}")
        if not use_quantization:
            print("ğŸ’¡ æç¤º: ä½¿ç”¨ python Arduino_Test/generate_arduino_files.py ç”ŸæˆArduinoå¤´æ–‡ä»¶")
        else:
            print("ğŸ’¡ æç¤º: å¦‚æœESP32è¾“å‡ºå›ºå®šå€¼ï¼Œå»ºè®®ä½¿ç”¨é»˜è®¤çš„Float32éé‡åŒ–æ¨¡å¼")
    else:
        print("\nâœ— è½¬æ¢å¤±è´¥")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 